{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c52a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "df_sp_sample = pd.read_csv(\"movie_sample.csv\", encoding=\"utf-8\")\n",
    "\n",
    "def clean_text(text):\n",
    "    from konlpy.tag import Okt\n",
    "    import re\n",
    "    okt = Okt()\n",
    "    text = re.sub(r\"[^가-힣\\s]\", \"\", str(text))  # 한글과 공백만 남기기\n",
    "    text = text.strip()\n",
    "    tokens = okt.morphs(text, stem=True)  # 형태소 분석(어간추출)\n",
    "    stopwords = set(['은', '는', '이', '가', '을', '를', '의', '에', '과', '와', '도', '한'])\n",
    "    return \" \".join([word for word in tokens if word not in stopwords])\n",
    "\n",
    "df_sp_sample[\"document\"] = df_sp_sample[\"document\"].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# 2. 학습 및 검증 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_sp_sample[\"document\"], df_sp_sample[\"label\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3c5cf",
   "metadata": {},
   "source": [
    "- keras \n",
    "    - pad_sequences()\n",
    "        - maxlen - 패딩 후 최대 길이\n",
    "        - dtype - 데이터 타입 지정\n",
    "        - padding - (pre, post)- 앞과 뒤 중 어디에 0을 추가할지\n",
    "        - truncating - pre, post 앞과 뒤 중 어디를 길이가 초과할 때 자를지?\n",
    "        - value - 값 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d166acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이징 패딩 처리\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b242546",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq=tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq=tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = max(len(seq) for seq in X_train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d56b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad=pad_sequences(X_train_seq, maxlen= max_len, padding='post')\n",
    "X_test_pad=pad_sequences(X_test_seq, maxlen= max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31bb89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 레이블인코딩과 원핫 인코딩으로 변환 \n",
    "encoder =LabelEncoder()\n",
    "y_train_enc=encoder.fit_transform(y_train)\n",
    "y_test_enc=encoder.transform(y_test)\n",
    "y_train_cat=to_categorical(y_train_enc)\n",
    "y_test_cat=to_categorical(y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29197ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 29ms/step - accuracy: 0.5012 - loss: 0.6940 - val_accuracy: 0.5080 - val_loss: 0.6931\n",
      "Epoch 2/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.4997 - loss: 0.6935 - val_accuracy: 0.4920 - val_loss: 0.6935\n",
      "Epoch 3/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.4944 - loss: 0.6934 - val_accuracy: 0.5080 - val_loss: 0.6931\n",
      "Epoch 4/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.5024 - loss: 0.6933 - val_accuracy: 0.5080 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.4976 - loss: 0.6932 - val_accuracy: 0.5080 - val_loss: 0.6933\n",
      "Epoch 6/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 29ms/step - accuracy: 0.4967 - loss: 0.6936 - val_accuracy: 0.4920 - val_loss: 0.6937\n",
      "Epoch 7/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.5019 - loss: 0.6933 - val_accuracy: 0.4920 - val_loss: 0.6935\n",
      "Epoch 8/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.5032 - loss: 0.6932 - val_accuracy: 0.4920 - val_loss: 0.6932\n",
      "Epoch 9/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.4994 - loss: 0.6932 - val_accuracy: 0.4920 - val_loss: 0.6936\n",
      "Epoch 10/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.5090 - loss: 0.6930 - val_accuracy: 0.5080 - val_loss: 0.6931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2c145fcb950>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## keras 딥러닝 모델을 간단하게 구현해서 만들 예정 \n",
    "## 입력층\n",
    "## 특징을 추출하는 layer\n",
    "## 정규화 기타 일반화 층 작업 Dropout 등등\n",
    "## 출력층\n",
    "## base model 만들기\n",
    "model = Sequential([\n",
    "    Embedding(input_dim = vocab_size, output_dim = 100, input_length = max_len),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "## model compile\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "## model 학습\n",
    "\n",
    "model.fit(X_train_pad, y_train_cat, epochs =10, batch_size=32, validation_data=(X_test_pad, y_test_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39dfa21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 31ms/step - accuracy: 0.5030 - loss: 0.6941 - val_accuracy: 0.4920 - val_loss: 0.6932\n",
      "Epoch 2/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.4920 - val_loss: 0.6933\n",
      "Epoch 3/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.5019 - loss: 0.6932 - val_accuracy: 0.4920 - val_loss: 0.6932\n",
      "Epoch 4/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.5068 - loss: 0.6931 - val_accuracy: 0.5080 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.5048 - loss: 0.6932 - val_accuracy: 0.4920 - val_loss: 0.6933\n",
      "Epoch 6/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.5050 - loss: 0.6932 - val_accuracy: 0.4920 - val_loss: 0.6936\n",
      "Epoch 7/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.5136 - loss: 0.6929 - val_accuracy: 0.5080 - val_loss: 0.6931\n",
      "Epoch 8/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.4996 - loss: 0.6932 - val_accuracy: 0.4920 - val_loss: 0.6934\n",
      "Epoch 9/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.5060 - loss: 0.6931 - val_accuracy: 0.4920 - val_loss: 0.6933\n",
      "Epoch 10/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.5047 - loss: 0.6932 - val_accuracy: 0.4920 - val_loss: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2c1948b7fe0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dropout의 비율을 조정\n",
    "\n",
    "model_dev = Sequential([\n",
    "    Embedding(input_dim = vocab_size, output_dim = 100, input_length = max_len),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "## model compile\n",
    "\n",
    "model_dev.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "## model 학습\n",
    "\n",
    "model_dev.fit(X_train_pad, y_train_cat, epochs =10, batch_size=32, validation_data=(X_test_pad, y_test_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68cb2397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 39ms/step - accuracy: 0.4958 - loss: 0.6972 - val_accuracy: 0.4918 - val_loss: 0.6935\n",
      "Epoch 2/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.4958 - loss: 0.6933 - val_accuracy: 0.4920 - val_loss: 0.7371\n",
      "Epoch 3/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.5070 - loss: 0.6930 - val_accuracy: 0.4920 - val_loss: 0.6952\n",
      "Epoch 4/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.4998 - loss: 0.6930 - val_accuracy: 0.5097 - val_loss: 0.6928\n",
      "Epoch 5/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.4945 - loss: 0.6935 - val_accuracy: 0.4920 - val_loss: 0.6929\n",
      "Epoch 6/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.5031 - loss: 0.6930 - val_accuracy: 0.4920 - val_loss: 0.6933\n",
      "Epoch 7/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.4974 - loss: 0.6931 - val_accuracy: 0.4920 - val_loss: 0.6886\n",
      "Epoch 8/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.5080 - loss: 0.6887 - val_accuracy: 0.5618 - val_loss: 0.6713\n",
      "Epoch 9/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.5545 - loss: 0.6722 - val_accuracy: 0.5235 - val_loss: 0.6864\n",
      "Epoch 10/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.5254 - loss: 0.6854 - val_accuracy: 0.6163 - val_loss: 0.6688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2c19912cc80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "## Dropout 비율 조정  & 배치정규화 추가 \n",
    "\n",
    "model_dev2 = Sequential([\n",
    "    Embedding(input_dim = vocab_size, output_dim = 200, input_length = max_len),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    BatchNormalization(), #배치 정규화 추가 \n",
    "    LSTM(32),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "## model compile\n",
    "\n",
    "model_dev2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "## model 학습\n",
    "\n",
    "model_dev2.fit(X_train_pad, y_train_cat, epochs =10, batch_size=32, validation_data=(X_test_pad, y_test_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d05ff3",
   "metadata": {},
   "source": [
    "# 필수과제\n",
    "## 성능을 개선하기 위해 한 작업\n",
    "- **1. model을 통해 성능을 개선해 보자!**\n",
    "    - model 두 가지 작업을 추가 \n",
    "        - Dropout 비율 조정  & 배치정규화 추가 \n",
    "        - 다른 레이어를 추가하거나 model을 더욱 고도화해서 만들면 성능 분명 개선될 것\n",
    "        - accuracy 0.85 이상 만들기 ( 0.85를 넘지 않으면 당연히 과제를 수행하지 않은 것 입니다. ) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5053fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 27ms/step - accuracy: 0.5006 - loss: 0.6938 - val_accuracy: 0.5080 - val_loss: 1.1304\n",
      "Epoch 2/4\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 26ms/step - accuracy: 0.7342 - loss: 0.5135 - val_accuracy: 0.8200 - val_loss: 0.4335\n",
      "Epoch 3/4\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 26ms/step - accuracy: 0.8799 - loss: 0.2984 - val_accuracy: 0.7678 - val_loss: 0.6096\n",
      "Epoch 4/4\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 27ms/step - accuracy: 0.9215 - loss: 0.2091 - val_accuracy: 0.8220 - val_loss: 0.4701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2c1a25a4f50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dev = Sequential([\n",
    "    Embedding(input_dim = vocab_size, output_dim = 100, input_length = max_len),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.45),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "## model compile\n",
    "\n",
    "model_dev.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "## model 학습\n",
    "\n",
    "model_dev.fit(X_train_pad, y_train_cat, epochs =4, batch_size=16, validation_data=(X_test_pad, y_test_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001d538",
   "metadata": {},
   "source": [
    "- **2. embedding**\n",
    "    - model 최적으로 고정한 상태로 \n",
    "    - padding(keras 내에 조절 가능), tf_idf, word2vec, 임베딩 최소 3개 이상을 비교해서 어떤 식으로 진행하는 게 가장 좋은 성능 보여주는지?\n",
    "    - 각 임베딩별로 성능이 어떻게 나오는지? \n",
    "        - 최적의 임베딩은 어떤 것인지 정리해 보고 \n",
    "        - 왜 다른 것들과 비교했을 때 성능이 좋은지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df95c7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 변환 후 차원: (24000, 17651)\n",
      "LSTM 입력을 위한 차원 확장: (24000, 1, 17651)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()  \n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()  \n",
    "\n",
    "X_train_tfidf_seq = np.expand_dims(X_train_tfidf, axis=1)  \n",
    "X_test_tfidf_seq = np.expand_dims(X_test_tfidf, axis=1)\n",
    "\n",
    "print(\"TF-IDF 변환 후 차원:\", X_train_tfidf.shape)  \n",
    "print(\"LSTM 입력을 위한 차원 확장:\", X_train_tfidf_seq.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3b77bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_eval_lstm_tfidf(X_train, X_test, y_train, y_test):\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=(1, X_train.shape[2])),\n",
    "        LSTM(32),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.45),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(y_train.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ed382ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec 임베딩 행렬 크기: (18690, 100)\n",
      "Word2Vec 변환 후 데이터 크기: (24000, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_tokens = [sentence.split() for sentence in X_train]\n",
    "X_test_tokens = [sentence.split() for sentence in X_test]\n",
    "w2v_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "w2v_model.train(X_train_tokens, total_examples=len(X_train_tokens), epochs=10)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "print(\"Word2Vec 임베딩 행렬 크기:\", embedding_matrix.shape)\n",
    "\n",
    "def sentence_vector(tokens, model, vector_size=100):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "X_train_w2v = np.array([sentence_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v = np.array([sentence_vector(tokens, w2v_model) for tokens in X_test_tokens])\n",
    "\n",
    "print(\"Word2Vec 변환 후 데이터 크기:\", X_train_w2v.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5934a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_lstm(X_train, X_test, y_train, y_test, embedding_matrix=None, trainable=True):\n",
    "    model = Sequential()\n",
    "\n",
    "    if embedding_matrix is None: \n",
    "        model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_len))\n",
    "    else: \n",
    "        model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=max_len, trainable=trainable))\n",
    "    \n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cde5a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
      "LSTM Accuracy - Keras: 0.8117\n"
     ]
    }
   ],
   "source": [
    "acc_keras = train_eval_lstm(X_train_pad, X_test_pad, y_train_cat, y_test_cat)\n",
    "print(f\"LSTM Accuracy - Keras: {acc_keras:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17f71be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step\n",
      "LSTM Accuracy - Word2Vec: 0.5582\n"
     ]
    }
   ],
   "source": [
    "acc_w2v = train_eval_lstm(X_train_w2v, X_test_w2v, y_train_cat, y_test_cat, embedding_matrix=embedding_matrix)\n",
    "print(f\"LSTM Accuracy - Word2Vec: {acc_w2v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eacbc6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "LSTM Accuracy - TF-IDF: 0.7705\n"
     ]
    }
   ],
   "source": [
    "acc_tfidf = train_eval_lstm_tfidf(X_train_tfidf_seq, X_test_tfidf_seq, y_train_cat, y_test_cat)\n",
    "print(f\"LSTM Accuracy - TF-IDF: {acc_tfidf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c684e16",
   "metadata": {},
   "source": [
    "### 결론\n",
    "- 최적 임베딩 방식 : **Keras 임베딩 (0.8117)**\n",
    "- 이유 : 데이터셋을 기반으로 학습하며 최적의 임베딩 벡터를 찾아서 \n",
    "    - Word2Vec : 데이터셋이 작고 도메인이 특화될 경우 성능 떨어짐\n",
    "    - TF-IDF : 단어·역문서 빈도 가중치로 인해 성능이 좋으나, 단어 간 문맥 고려 제한"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8e249",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
