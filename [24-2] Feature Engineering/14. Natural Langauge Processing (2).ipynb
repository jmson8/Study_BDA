{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e2aaea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(\"movie_sample.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^가-힣\\s]\", \"\", str(text))  # 한글과 공백만 남기기\n",
    "    return text.strip()\n",
    "\n",
    "df[\"document\"] = df[\"document\"].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# 학습, 검증 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"document\"], df[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "89819b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model은 Logistic으로 고정\n",
    "#============================\n",
    "# 1. One-hot Encoding\n",
    "#============================\n",
    "vectorizer_onehot = CountVectorizer(binary=True)\n",
    "X_train_onehot=vectorizer_onehot.fit_transform(X_train)\n",
    "X_test_onehot=vectorizer_onehot.transform(X_test)\n",
    "#============================\n",
    "# 2. Count Vectorizer\n",
    "#============================\n",
    "vectorizer_count = CountVectorizer()\n",
    "X_train_count = vectorizer_count.fit_transform(X_train)\n",
    "X_test_count = vectorizer_count.transform(X_test)\n",
    "#============================\n",
    "# 3. TF-IDF\n",
    "#============================\n",
    "vectorizer_tfidf=TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "#============================\n",
    "# 4. Word2Vec\n",
    "#============================\n",
    "X_train_tokens = [sentence.split() for sentence in X_train]\n",
    "X_test_tokens = [sentence.split() for sentence in X_test]\n",
    "\n",
    "w2v_model = Word2Vec(sentences = X_train_tokens, vector_size = 100, window=5, min_count=1)\n",
    "w2v_model.train(X_train_tokens, total_examples = len(X_train_tokens), epochs=10)\n",
    "\n",
    "def sentence_vector(tokens, model, vector_size= 100):\n",
    "    vectors =[model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis =0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "X_train_w2v =np.array([sentence_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v =np.array([sentence_vector(tokens, w2v_model) for tokens in X_test_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31189441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.7566\n",
      "count:0.7553\n",
      "tfidf:0.7563\n",
      "Word2Vec:0.6390\n"
     ]
    }
   ],
   "source": [
    "## 각 임베딩별로 성능 평가\n",
    "\n",
    "def train_eval_model(X_train, X_test, y_train, y_test):\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "## 임베딩별로 성능평가\n",
    "\n",
    "result={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test)       \n",
    "}\n",
    "\n",
    "#성능비교 결과\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b317eed3",
   "metadata": {},
   "source": [
    "### (방법1) 성능을 올리기 위해서는?\n",
    "- 형태소 분석 추가 ( 어간 추출 추가 )\n",
    "- 한국어 불용어 제거 ( 단어 사전을 사용하진 않음 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4bbefab0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m은\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m는\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m가\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m을\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m를\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m의\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m에\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m과\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m와\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m도\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m한\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords])\n\u001b[1;32m---> 14\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 학습, 검증 데이터 분리\u001b[39;00m\n\u001b[0;32m     17\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[91], line 9\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^가-힣\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(text))  \u001b[38;5;66;03m# 한글과 공백만 남기기\u001b[39;00m\n\u001b[0;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m----> 9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m okt\u001b[38;5;241m.\u001b[39mmorphs(text, stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#형태소 분석(어간추출)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m## 불용어 제거\u001b[39;00m\n\u001b[0;32m     11\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m은\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m는\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m가\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m을\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m를\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m의\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m에\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m과\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m와\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m도\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m한\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\konlpy\\tag\\_okt.py:89\u001b[0m, in \u001b[0;36mOkt.morphs\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmorphs\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos(phrase, norm\u001b[38;5;241m=\u001b[39mnorm, stem\u001b[38;5;241m=\u001b[39mstem)]\n",
      "File \u001b[1;32mc:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[1;32m---> 71\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjki\u001b[38;5;241m.\u001b[39mtokenize(\n\u001b[0;32m     72\u001b[0m             phrase,\n\u001b[0;32m     73\u001b[0m             jpype\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mBoolean(norm),\n\u001b[0;32m     74\u001b[0m             jpype\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mBoolean(stem))\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"movie_sample.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def clean_text(text):\n",
    "    okt = Okt()\n",
    "    text = re.sub(r\"[^가-힣\\s]\", \"\", str(text))  # 한글과 공백만 남기기\n",
    "    text = text.strip()\n",
    "    tokens = okt.morphs(text, stem=True) #형태소 분석(어간추출)\n",
    "    ## 불용어 제거\n",
    "    stopwords = set(['은','는','가','을','를','의','에','과','와','도','한'])\n",
    "    return \" \".join([word for word in tokens if word not in stopwords])\n",
    "\n",
    "df[\"document\"] = df[\"document\"].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# 학습, 검증 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"document\"], df[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c72017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model은 Logistic으로 고정\n",
    "#============================\n",
    "# 1. One-hot Encoding\n",
    "#============================\n",
    "vectorizer_onehot = CountVectorizer(binary=True)\n",
    "X_train_onehot=vectorizer_onehot.fit_transform(X_train)\n",
    "X_test_onehot=vectorizer_onehot.transform(X_test)\n",
    "#============================\n",
    "# 2. Count Vectorizer\n",
    "#============================\n",
    "vectorizer_count = CountVectorizer()\n",
    "X_train_count = vectorizer_count.fit_transform(X_train)\n",
    "X_test_count = vectorizer_count.transform(X_test)\n",
    "#============================\n",
    "# 3. TF-IDF\n",
    "#============================\n",
    "vectorizer_tfidf=TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "#============================\n",
    "# 4. Word2Vec\n",
    "#============================\n",
    "X_train_tokens = [sentence.split() for sentence in X_train]\n",
    "X_test_tokens = [sentence.split() for sentence in X_test]\n",
    "\n",
    "w2v_model = Word2Vec(sentences = X_train_tokens, vector_size = 100, window=5, min_count=1)\n",
    "w2v_model.train(X_train_tokens, total_examples = len(X_train_tokens), epochs=10)\n",
    "\n",
    "def sentence_vector(tokens, model, vector_size= 100):\n",
    "    vectors =[model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis =0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "X_train_w2v =np.array([sentence_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v =np.array([sentence_vector(tokens, w2v_model) for tokens in X_test_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8159073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.8047\n",
      "count:0.8042\n",
      "tfidf:0.8077\n",
      "Word2Vec:0.7753\n"
     ]
    }
   ],
   "source": [
    "## 임베딩별로 성능평가\n",
    "\n",
    "result_rvs1={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test)       \n",
    "}\n",
    "\n",
    "#성능비교 결과\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs1.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f172d",
   "metadata": {},
   "source": [
    "### (방법2) 하이퍼파라미터 추가\n",
    "- TF-IDF 성능을 좀 더 개선 시킬 수 있는 방법은?\n",
    "- TFIDF를 하이퍼파라미터를 추가해서 성능을 더 개선해 볼까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082d1459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model은 Logistic으로 고정\n",
    "#============================\n",
    "# 1. One-hot Encoding\n",
    "#============================\n",
    "vectorizer_onehot = CountVectorizer(binary=True)\n",
    "X_train_onehot=vectorizer_onehot.fit_transform(X_train)\n",
    "X_test_onehot=vectorizer_onehot.transform(X_test)\n",
    "#============================\n",
    "# 2. Count Vectorizer\n",
    "#============================\n",
    "vectorizer_count = CountVectorizer()\n",
    "X_train_count = vectorizer_count.fit_transform(X_train)\n",
    "X_test_count = vectorizer_count.transform(X_test)\n",
    "#============================\n",
    "# 3. TF-IDF\n",
    "#============================\n",
    "vectorizer_tfidf=TfidfVectorizer(min_df=3, max_df=0.85, ngram_range=(1,3))\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "#============================\n",
    "# 4. Word2Vec\n",
    "#============================\n",
    "X_train_tokens = [sentence.split() for sentence in X_train]\n",
    "X_test_tokens = [sentence.split() for sentence in X_test]\n",
    "\n",
    "w2v_model = Word2Vec(sentences = X_train_tokens, vector_size = 100, window=5, min_count=1)\n",
    "w2v_model.train(X_train_tokens, total_examples = len(X_train_tokens), epochs=10)\n",
    "\n",
    "def sentence_vector(tokens, model, vector_size= 100):\n",
    "    vectors =[model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis =0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "X_train_w2v =np.array([sentence_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v =np.array([sentence_vector(tokens, w2v_model) for tokens in X_test_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2774f0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.8047\n",
      "count:0.8042\n",
      "tfidf:0.8159\n",
      "Word2Vec:0.7759\n"
     ]
    }
   ],
   "source": [
    "result_rvs2={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test)       \n",
    "}\n",
    "\n",
    "#성능비교 결과\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs2.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b59466",
   "metadata": {},
   "source": [
    "- base 전처리 (tfidf) : 0.7662\n",
    "- Model 텍스트 전처리 (tfidf) : 0.8077\n",
    "- TFIDF 하이퍼파라미터 튜닝 : 0.8159"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b51bc5",
   "metadata": {},
   "source": [
    "### (방법3) Model을 여러 개 섞어서 성능을 개선시키자!\n",
    "\n",
    "- voting(xgb, randomforest, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f11a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ec3863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:05:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.8047\n",
      "count:0.8042\n",
      "tfidf:0.8159\n",
      "Word2Vec:0.7759\n",
      "Ensemble:0.7953\n"
     ]
    }
   ],
   "source": [
    "## Voting추가하여 성능평가\n",
    "log_rg = LogisticRegression(max_iter=1000)\n",
    "rf_clf= RandomForestClassifier(n_estimators=100)\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('lr',log_rg),\n",
    "    ('rf',rf_clf),\n",
    "    ('xgb',xgb_clf)\n",
    "],voting='hard')\n",
    "\n",
    "ensemble_model.fit(X_train_tfidf,y_train)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test_tfidf)\n",
    "ensemble_accuracy=accuracy_score(y_test, y_pred_ensemble)\n",
    "#성능비교 결과\n",
    "\n",
    "result_rvs3={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test),\n",
    "    'Ensemble': ensemble_accuracy\n",
    "}\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs3.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110782a",
   "metadata": {},
   "source": [
    "## 필수과제1\n",
    "- 정규식, 형태소 등의 전처리를 사용 및 TFIDF 하이퍼파라미터 튜닝 -> tfidf:0.8190 가 나왔음 \n",
    "- word2vec 튜닝 작업 진행해서 -> 0.8190보다 성능이 더 높게 나와야 합니다.\n",
    "    - 모델의 앙상블 형태의 결합 등을 같이 이용해서 word2vec의 전처리로만 성능이 0.8190가 넘으면 됩니다.\n",
    "- 0.8190 넘지 않으면 과제 미제출(Word2vec)\n",
    "- **만약 성능이 너무 개선되지 않으면 데이터셋을 추가로 더 사용하셔도 됩니다!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f015d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"movie_sample.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def clean_text(text):\n",
    "    okt = Okt()\n",
    "    text = re.sub(r\"[^가-힣\\s]\", \"\", str(text))  # 한글과 공백만 남기기\n",
    "    text = text.strip()\n",
    "    tokens = okt.morphs(text, stem=True) #형태소 분석(어간추출)\n",
    "    ## 불용어 제거\n",
    "    stopwords = set(['은','는','가','을','를','의','에','과','와','도','한'])\n",
    "    return \" \".join([word for word in tokens if word not in stopwords])\n",
    "\n",
    "df[\"document\"] = df[\"document\"].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# 학습, 검증 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"document\"], df[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "866f3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = [sentence.split() for sentence in X_train]\n",
    "X_test_tokens = [sentence.split() for sentence in X_test]\n",
    "\n",
    "w2v_model = Word2Vec(sentences = X_train_tokens, vector_size = 300, window=10, min_count=5)\n",
    "w2v_model.train(X_train_tokens, total_examples = len(X_train_tokens), epochs=30)\n",
    "\n",
    "def sentence_vector(tokens, model, vector_size= 300):\n",
    "    vectors =[model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis =0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "X_train_w2v =np.array([sentence_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v =np.array([sentence_vector(tokens, w2v_model) for tokens in X_test_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac82382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.8047\n",
      "count:0.8042\n",
      "tfidf:0.8159\n",
      "Word2Vec:0.7930\n"
     ]
    }
   ],
   "source": [
    "result_rvs4={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test)       \n",
    "}\n",
    "\n",
    "#성능비교 결과\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs4.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f20df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = [sentence.split() for sentence in X_train]\n",
    "X_test_tokens = [sentence.split() for sentence in X_test]\n",
    "\n",
    "w2v_model_sg = Word2Vec(sentences = X_train_tokens, vector_size = 300, window=10, min_count=5, sg=1)\n",
    "w2v_model_sg.train(X_train_tokens, total_examples = len(X_train_tokens), epochs=30)\n",
    "\n",
    "def sentence_vector(tokens, model, vector_size= 300):\n",
    "    vectors =[model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis =0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "X_train_w2v =np.array([sentence_vector(tokens, w2v_model_sg) for tokens in X_train_tokens])\n",
    "X_test_w2v =np.array([sentence_vector(tokens, w2v_model_sg) for tokens in X_test_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f88e58dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.8047\n",
      "count:0.8042\n",
      "tfidf:0.8159\n",
      "Word2Vec:0.8047\n"
     ]
    }
   ],
   "source": [
    "result_rvs5={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test)       \n",
    "}\n",
    "\n",
    "#성능비교 결과\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs5.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bcdca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:25:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.8047\n",
      "count:0.8042\n",
      "tfidf:0.8159\n",
      "Word2Vec:0.8047\n",
      "Ensemble:0.8068\n"
     ]
    }
   ],
   "source": [
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('lr',log_rg),\n",
    "    ('rf',rf_clf),\n",
    "    ('xgb',xgb_clf)\n",
    "],voting='hard')\n",
    "\n",
    "ensemble_model.fit(X_train_w2v,y_train)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test_w2v)\n",
    "ensemble_accuracy=accuracy_score(y_test, y_pred_ensemble)\n",
    "#성능비교 결과\n",
    "\n",
    "result_rvs6={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test),\n",
    "    'Ensemble': ensemble_accuracy\n",
    "}\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs6.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1b4790cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"movie_sample.csv\", encoding=\"utf-8\")\n",
    "\n",
    "def clean_text(text):\n",
    "    okt = Okt()\n",
    "    \n",
    "    # 특수문자, 숫자 제거, 공백 정리\n",
    "    text = re.sub(r\"[^가-힣\\s]\", \"\", str(text))  \n",
    "    text = re.sub(r\"\\d+\", \"\", text)  \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  \n",
    "    \n",
    "    # 형태소 분석 (어간 추출 적용)\n",
    "    tokens = okt.morphs(text, stem=True)  \n",
    "    \n",
    "    # 불용어 제거 \n",
    "    stopwords = set([\n",
    "        '은', '는', '이', '가', '을', '를', '의', '에', '과', '와', '도', '한',\n",
    "        '에서', '부터', '까지', '하고', '보다', '인데', '때문', '였다', '다면', '처럼'\n",
    "    ])\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 데이터 적용\n",
    "df[\"document\"] = df[\"document\"].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# 학습, 검증 데이터 분리s\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"document\"], df[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdf38836",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = [sentence.split() for sentence in X_train]\n",
    "X_test_tokens = [sentence.split() for sentence in X_test]\n",
    "\n",
    "w2v_model_sg = Word2Vec(sentences = X_train_tokens, vector_size = 300, window=10, min_count=5, sg=1)\n",
    "w2v_model_sg.train(X_train_tokens, total_examples = len(X_train_tokens), epochs=30)\n",
    "\n",
    "def sentence_vector(tokens, model, vector_size= 300):\n",
    "    vectors =[model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis =0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "X_train_w2v =np.array([sentence_vector(tokens, w2v_model_sg) for tokens in X_train_tokens])\n",
    "X_test_w2v =np.array([sentence_vector(tokens, w2v_model_sg) for tokens in X_test_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2f8d5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.8047\n",
      "count:0.8042\n",
      "tfidf:0.8159\n",
      "Word2Vec:0.8047\n"
     ]
    }
   ],
   "source": [
    "result_rvs7={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test)       \n",
    "}\n",
    "\n",
    "#성능비교 결과\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs7.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "993cca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsm53\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:36:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.8047\n",
      "count:0.8042\n",
      "tfidf:0.8159\n",
      "Word2Vec:0.8047\n",
      "Ensemble:0.8079\n"
     ]
    }
   ],
   "source": [
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('lr',log_rg),\n",
    "    ('rf',rf_clf),\n",
    "    ('xgb',xgb_clf)\n",
    "],voting='hard')\n",
    "\n",
    "ensemble_model.fit(X_train_w2v,y_train)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test_w2v)\n",
    "ensemble_accuracy=accuracy_score(y_test, y_pred_ensemble)\n",
    "#성능비교 결과\n",
    "\n",
    "result_rvs8={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test),\n",
    "    'Ensemble': ensemble_accuracy\n",
    "}\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs8.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76a2c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = [sentence.split() for sentence in X_train]\n",
    "X_test_tokens = [sentence.split() for sentence in X_test]\n",
    "\n",
    "w2v_model_sg = Word2Vec(sentences = X_train_tokens, vector_size = 300, window=10, min_count=5, sg=1)\n",
    "w2v_model_sg.train(X_train_tokens, total_examples = len(X_train_tokens), epochs=30)\n",
    "\n",
    "def sentence_vector(tokens, model, vector_size= 300):\n",
    "    vectors =[model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis =0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "X_train_w2v =np.array([sentence_vector(tokens, w2v_model_sg) for tokens in X_train_tokens])\n",
    "X_test_w2v =np.array([sentence_vector(tokens, w2v_model_sg) for tokens in X_test_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20c27a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.8047\n",
      "count:0.8042\n",
      "tfidf:0.8159\n",
      "Word2Vec:0.8056\n"
     ]
    }
   ],
   "source": [
    "result_rvs8={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test)       \n",
    "}\n",
    "\n",
    "#성능비교 결과\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs8.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd5ceddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.8047\n",
      "count:0.8042\n",
      "tfidf:0.8159\n",
      "Word2Vec:0.8056\n",
      "Ensemble:0.8113\n"
     ]
    }
   ],
   "source": [
    "log_rg = LogisticRegression(C=5.0, max_iter=1000)\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, max_depth=20, random_state=1)\n",
    "xgb_clf = XGBClassifier(learning_rate=0.05, n_estimators=500, max_depth=6, random_state=1)\n",
    "\n",
    "# 최적화된 앙상블 모델\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('lr', log_rg),\n",
    "    ('rf', rf_clf),\n",
    "    ('xgb', xgb_clf)\n",
    "], voting='hard')\n",
    "\n",
    "# 학습 및 평가\n",
    "ensemble_model.fit(X_train_w2v, y_train)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test_w2v)\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "\n",
    "result_rvs9={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test),\n",
    "    'Ensemble': ensemble_accuracy\n",
    "}\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs9.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0b8a6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = [sentence.split() for sentence in X_train]\n",
    "X_test_tokens = [sentence.split() for sentence in X_test]\n",
    "\n",
    "w2v_model_sg = Word2Vec(sentences = X_train_tokens, vector_size = 500, window=20, min_count=3, sg=1, negative=15, sample=0.0001, epochs=100)\n",
    "w2v_model_sg.train(X_train_tokens, total_examples = len(X_train_tokens), epochs=100)\n",
    "\n",
    "def sentence_vector(tokens, model, vector_size= 500):\n",
    "    vectors =[model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis =0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "X_train_w2v =np.array([sentence_vector(tokens, w2v_model_sg) for tokens in X_train_tokens])\n",
    "X_test_w2v =np.array([sentence_vector(tokens, w2v_model_sg) for tokens in X_test_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0fecaeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.7653\n",
      "count:0.7662\n",
      "tfidf:0.7675\n",
      "Word2Vec:0.8097\n"
     ]
    }
   ],
   "source": [
    "result_rvs10={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test)       \n",
    "}\n",
    "\n",
    "#성능 비교 결과\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs10.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "79f3040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.7653\n",
      "count:0.7662\n",
      "tfidf:0.7675\n",
      "Word2Vec:0.8097\n",
      "Ensemble:0.8150\n"
     ]
    }
   ],
   "source": [
    "log_rg = LogisticRegression(C=10.0, max_iter=1000)\n",
    "rf_clf = RandomForestClassifier(n_estimators=500, max_depth=None, random_state=1)\n",
    "xgb_clf = XGBClassifier(learning_rate=0.03, n_estimators=700, max_depth=8, random_state=1)\n",
    "\n",
    "# 최적화된 앙상블 모델\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('lr', log_rg),\n",
    "    ('rf', rf_clf),\n",
    "    ('xgb', xgb_clf)\n",
    "], voting='hard')\n",
    "\n",
    "ensemble_model.fit(X_train_w2v, y_train)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test_w2v)\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "\n",
    "result_rvs11={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test),\n",
    "    'Ensemble': ensemble_accuracy\n",
    "}\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs11.items():\n",
    "    print(f'{method}:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "00cd71dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11908, number of negative: 12092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127500\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496167 -> initscore=-0.015334\n",
      "[LightGBM] [Info] Start training from score -0.015334\n",
      "Model 임베딩별 성능 결과\n",
      "one-hot:0.7653\n",
      "count:0.7662\n",
      "tfidf:0.7675\n",
      "Word2Vec:0.8097\n",
      "Ensemble:0.8200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 최적화된 개별 모델\n",
    "log_rg = LogisticRegression(C=10.0, max_iter=1000)\n",
    "rf_clf = RandomForestClassifier(n_estimators=500, max_depth=6, n_jobs=-1, random_state=1)\n",
    "xgb_clf = XGBClassifier(learning_rate=0.05, n_estimators=500, max_depth=6, n_jobs=-1, random_state=1)\n",
    "lgb_clf = LGBMClassifier(learning_rate=0.05, n_estimators=500, max_depth=6, n_jobs=-1, random_state=1)\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('lr', log_rg),\n",
    "    ('rf', rf_clf),\n",
    "    ('xgb', xgb_clf),\n",
    "    ('lgb', lgb_clf),\n",
    "], voting='soft')\n",
    "\n",
    "# 학습 및 평가\n",
    "ensemble_model.fit(X_train_w2v, y_train)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test_w2v)\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "\n",
    "result_rvs12={\n",
    "    'one-hot' : train_eval_model(X_train_onehot, X_test_onehot, y_train, y_test),\n",
    "    'count' : train_eval_model(X_train_count, X_test_count, y_train, y_test),\n",
    "    'tfidf' : train_eval_model(X_train_tfidf, X_test_tfidf, y_train, y_test),    \n",
    "    'Word2Vec' : train_eval_model(X_train_w2v, X_test_w2v, y_train, y_test),\n",
    "    'Ensemble': ensemble_accuracy\n",
    "}\n",
    "\n",
    "print('Model 임베딩별 성능 결과')\n",
    "for method,accuracy in result_rvs12.items():\n",
    "    print(f'{method}:{accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
